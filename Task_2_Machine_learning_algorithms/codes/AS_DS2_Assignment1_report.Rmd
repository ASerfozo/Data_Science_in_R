---
title: "Data Science 1 - Assignment 1"
author: "Attila Serfozo"
date: '2021.03.18 '
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
library(tidyverse)
library(caret)
library(rpart)
library(rpart.plot)
library(pROC)
library(viridis)
library(gbm)

options(digits = 4)
```

# 1. Tree ensemble models

In the first exercise we are going to work with the OJ dataset, which records purchases of two types of orange juices (Citrus Hill and Minute Mad) and presents customer and product characteristics as features. The goal is to predict which of the juices is chosen in a given purchase situation.

### a. Create a training data of 75% and keep 25% of the data as a test set. Train a decision tree as a benchmark model. Plot the final model and interpret the result (using rpart and rpart.plot is an easier option).

The plot of a basic decision tree can be seen below. Based on the tree the most important predictor is customer loyalty towards Citrus Hill which appears at many cuts of the decision tree.

```{r echo=FALSE, warning=FALSE, message=FALSE}

data <- as_tibble(ISLR::OJ)
# str(data)
# skimr::skim(data)

set.seed(20210318)

# Separate holdout set
train_indices <- as.integer(createDataPartition(data$Purchase, p = 0.75, list = FALSE))
data_train <- data[train_indices, ]
data_holdout <- data[-train_indices, ]

# 5-fold CV
train_control <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = TRUE,
  verboseIter = FALSE
)

set.seed(20210318)
benchmark_model <- train(
  Purchase ~ .,
  data = data_train,
  method = "rpart",
  metric = "ROC",
  trControl = train_control,
  tuneGrid= expand.grid(cp = 0.0005)
  )

rpart.plot(benchmark_model$finalModel, tweak = 1.5, main = "Benchmark model ( CART)")

```

### b. Investigate tree ensemble models: random forest, gradient boosting machine, XGBoost. Try various tuning parameter combinations and select the best model using cross-validation.

After running a cart model as benchmark, I had a look at random forest, gradient boosting machine and xgboost models. For random forest I tried different number of mtry parameters and node size, in the end the mtry of 8 and node size of 40 were the best tune parameters. 

```{r echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}

# Random Forest
rf_tune_grid <- expand.grid(
  .mtry = 2:10,
  .splitrule = "gini",
  .min.node.size = seq(5,50,by = 5) 
)

set.seed(20210318)
rf_model <- train(
  Purchase ~ .,
  data = data_train,
  method = "ranger",
  metric = "ROC",
  trControl = train_control,
  tuneGrid= rf_tune_grid,
  importance = "impurity"
  )

# Best tune parameter is mtry 8 and min node size 40
#rf_model$bestTune
plot(rf_model, main = "Random Forest tuning")
```

For GBM I tried different number of trees, level of depth, node size and shrinkage parameters. In the end the best ROC was with 100 trees, 5 interaction depth, 0.05 shrinkage and 10 min node size. 

```{r echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}

# Gradient Boosting Machine
gbm_tune_grid <- expand.grid(
  interaction.depth=c(1:5),                    # depth of trees
  n.trees=c(100, 200,300),                     # number of trees
  shrinkage = c(.0005, .001, 0.01, 0.05, 0.1), # learning rate
  n.minobsinnode = seq(10,20,by = 5)           # minimum node size
)

set.seed(20210318)
gbm_model <- train(Purchase ~ .,
                   data = data_train, 
                   method = "gbm",
                   verbose = FALSE,
                   metric = "ROC",
                   trControl=train_control,
                   tuneGrid = gbm_tune_grid)
# Best tune parameter is 100 trees, 5 depth, 0.05 shrinkage and 10 min node size
#gbm_model$bestTune
plot(gbm_model, main = "GBM model tuning")
```

Finally I tried XGBoost with different number of trees, max tree depth, learning rate and column sampling. In the end the 350 number of trees with 0.05 learning rate and 0.5 column subsampling were the best tune parameters.

```{r echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}

# XGBoost
xgb_grid <-  expand.grid(
  nrounds=c(350, 500),                   # Number of trees, default: 100
  max_depth = c(2,3,4),                  # Maximum tree depth, default: 6
  eta = c(0.03,0.05, 0.06),              # Learning rate, default: 0.3
  gamma = c(0.01),                       # Used for tuning of Regularization, default: 0
  colsample_bytree = seq(0.3, 0.5, 0.6), # Column sampling, default: 1
  subsample = c(0.75),                   # Row sampling, default: 1
  min_child_weight = c(0))               # Minimum leaf weight, default: 1

set.seed(20210318)
xgb_model <- train(
  Purchase ~ .,
  data = data_train,
  method = "xgbTree",
  metric = "ROC",
  tuneGrid = xgb_grid,
  trControl = train_control
)

#xgb_model$bestTune
plot(xgb_model, main = "XGB Model tuning")

```

### c. Compare the performance of the different models (if you use caret you should consider using the resamples function). Make sure to set the same seed before model training for all 3 models so that your cross validation samples are the same. Is any of these giving significantly different predictive power than the others?

As it can be seen below the Random Forest, GBM and XGBoost were both close regarding the ROC, but the GBM and XGBoost achieved to get above 90%, the GBM had the best results a slightly in front of the XGBoost. The benchmark (cart) model has a less significant prediction power compared to the others.

```{r echo=FALSE, warning=FALSE, message=FALSE}

final_models <-
  list("CART" = benchmark_model,
       "Random_Forest" = rf_model,
       "GBM" = gbm_model,
       "XGBoost" = xgb_model)

results <- resamples(final_models) %>% summary()
#results

results <- imap(final_models, ~{
  mean(round(results$values,4)[[paste0(.y,"~ROC")]])
}) %>% rbind()
#results
knitr::kable(results,caption="Models ROC")

```

### d. Choose the best model and plot ROC curve for the best model on the test set. Calculate and interpret AUC.

Below we can see the AUC of the models on the training set. The AUC of the GBM is the best with 89.26% area under curve meaning that more than 89% of the customers juice choice was predicted right by the GBM model.

```{r echo=FALSE, warning=FALSE, message=FALSE}


# Calculate models AUC
CV_AUC_folds <- list()

for (model_name in names(final_models)) {
  
  auc <- list()
  model <- final_models[[model_name]]
  for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
    cv_fold <-
      model$pred %>%
      filter(Resample == fold)
    
    roc_obj <- roc(cv_fold$obs, cv_fold$CH)
    auc[[fold]] <- round(as.numeric(roc_obj$auc),4)
  }
  
  CV_AUC_folds[[model_name]] <- data.frame("Resample" = names(auc),
                                           "AUC" = unlist(auc))
}

CV_AUC <- list()
for (model_name in names(final_models)) {
  CV_AUC[[model_name]] <- mean(CV_AUC_folds[[model_name]]$AUC)
}

models_AUC <- CV_AUC %>% rbind() 
# models_AUC
knitr::kable(models_AUC,caption="Models AUC")

```

The GBM model performed similarly well on the holdout set as well with an area under curve of 89.87%. The plot of the curve can be found below

```{r echo=FALSE, warning=FALSE, message=FALSE}


# ROC on test set

gbm_pred <- predict(gbm_model, data_holdout, type="prob")
# library(caTools)
# colAUC(gbm_pred, data_holdout$Purchase, plotROC = TRUE)


data_holdout[,"best_model_pred"] <- gbm_pred[,"CH"]

roc_obj_holdout <- roc(data_holdout$Purchase, data_holdout$best_model_pred)

# ROC plot creator function made by Gábor Békés 
createRocPlot <- function(r, plot_name) {
  all_coords <- coords(r, x="all", ret="all", transpose = FALSE)
  
  roc_plot <- ggplot(data = all_coords, aes(x = fpr, y = tpr)) +
    geom_line(color='blue', size = 0.7) +
    geom_area(aes(fill = 'red', alpha=0.4), alpha = 0.3, position = 'identity', color = 'blue') +
    scale_fill_viridis(discrete = TRUE, begin=0.6, alpha=0.5, guide = FALSE) +
    xlab("False Positive Rate (1-Specifity)") +
    ylab("True Positive Rate (Sensitivity)") +
    geom_abline(intercept = 0, slope = 1,  linetype = "dotted", col = "black") +
    scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1), expand = c(0, 0.01)) +
    scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .1), expand = c(0.01, 0)) + 
    theme_bw()
  
  roc_plot
}

createRocPlot(roc_obj_holdout, "ROC curve for best model (GBM) on the holdout set")

```

### e. Inspect variable importance plots for the 3 models. Are similar variables found to be the most important for the 3 models?

We can found the variable importance plot of the 3 models below. We can see that the top 5 most important variables are not the same for the models. Only the loyalty, price difference and week of purchase are the fix elements and in case of every model the loyalty is the number one predictor variable. Also I found it interesting that the SpecialCH variable is the 6th and 8th most important in the random forest and gbm while in the XGboost model it does not have a significant effect on prediction (16th in the line).

```{r echo=FALSE, warning=FALSE, message=FALSE}

plot(varImp(rf_model), main = "Random Forest - Variable Importance")

plot(varImp(gbm_model), main = "GBM - Variable Importance")

plot(varImp(xgb_model), main = "XGBoost - Variable Importance")

```


# 2. Variable importance profiles --------------------------------------------

In the second exercise we are going to work with the Hitters dataset, which records baseball players and their salaries. The goal is to check variable importance plots for interesting facts.

```{r echo=FALSE, warning=FALSE, message=FALSE}

data <- as_tibble(ISLR::Hitters) %>%
  drop_na(Salary) %>%
  mutate(log_salary = log(Salary), Salary = NULL)

```

### a. Train two random forest models: one with sampling 2 variables randomly for each split and one with 10 (use the whole dataset and don’t do cross-validation). Inspect variable importance profiles. What do you see in terms of how important the first few variables are relative to each other?

The variable plots of the two models can be found below. We can see that in case of the tree sampling 2 variables randomly the variable importance is well balanced. In contrary, in the tree sampling 10 variables 3 variables are over represented in the trees with high variable importance, while others are penalized with low importance.

```{r echo=FALSE, warning=FALSE, message=FALSE}

library(caret)

# Random Forest
tune_grid <- expand.grid(
  .mtry = 2,
  .splitrule = "variance",
  .min.node.size = c(5,10) 
)

set.seed(20210318)
rf_model_1 <- train(
  log_salary ~ .,
  data = data,
  method = "ranger",
  tuneGrid= tune_grid,
  importance = "impurity",   # need to be added to see varimp
  verbose = FALSE
)
rf_model_1

tune_grid <- expand.grid(
  .mtry = 10,
  .splitrule = "variance",
  .min.node.size = c(5,10) 
)

set.seed(20210318)
rf_model_2 <- train(
  log_salary ~ .,
  data = data,
  method = "ranger",
  tuneGrid= tune_grid,
  importance = "impurity",   # need to be added to see varimp
  verbose = FALSE
)
rf_model_2

plot(varImp(rf_model_1), main = "Random Forest sampling 2 variables - Variable Importance")
plot(varImp(rf_model_2), main = "Random Forest sampling 10 variables - Variable Importance")

```

### b. One of them is more extreme in terms of how the most important and the next ones relate to each other. Give an intuitive explanation how mtry/mtries relates to relative importance of variables in random forest models.

The model with less - 2 - mtry chooses from fewer number of variables, as a result the variable importance is more distributed. The model with 10 mtry value includes more variables at the cuts selection which means it more often chooses the CatBar variable.

### c. In the same vein, estimate two gbm models with varying rate of sampling for each tree (use 0.1 and 1 for the parameter bag.fraction/sample_rate). Hold all the other parameters fixed: grow 500 trees with maximum depths of 5, applying a learning rate of 0.1. Compare variable importance plots for the two models. Could you explain why is one variable importance profile more extreme than the other?

In case of GBM the different sampling rates means different number of rows included for modelling (lower subsample rate means fewer rows for tree growing), which results more distributed variable importance.

```{r echo=FALSE, warning=FALSE, message=FALSE}

library(gbm)

tune_grid <- expand.grid(
  interaction.depth=5,
  n.trees=500,
  shrinkage = 0.1,
  n.minobsinnode = 5
)

set.seed(20210318)
gbm_model_1 <- train(log_salary ~ .,
                     data = data, 
                     method = "gbm",
                     bag.fraction = 0.1,
                     verbose = FALSE,
                     tuneGrid = tune_grid)

set.seed(20210318)
gbm_model_2 <- train(log_salary ~ .,
                     data = data, 
                     method = "gbm",
                     bag.fraction = 1,
                     verbose = FALSE,
                     tuneGrid = tune_grid)

plot(varImp(gbm_model_1), main = "GBM with 0.1 rate of sampling - Variable Importance")
plot(varImp(gbm_model_2), main = "GBM with 1 rate of sampling - Variable Importance")

```

# 3. Stacking ----------------------------------------------------------------

Our goal on the third dataset is to predict whether a patient will actually show up for their medical appointments. For this exercise I will use h2o to do the predictions.

```{r message=FALSE, warning=FALSE, include=FALSE}

# Get Data
data <- read_csv("KaggleV2-May-2016.csv")

# some data cleaning
data <- select(data, -one_of(c("PatientId", "AppointmentID", "Neighbourhood"))) %>%
  janitor::clean_names()

# for binary prediction, the target variable must be a factor + generate new variables
data <- mutate(
  data,
  no_show = factor(no_show, levels = c("Yes", "No")),
  handcap = ifelse(handcap > 0, 1, 0),
  across(c(gender, scholarship, hipertension, alcoholism, handcap), factor),
  hours_since_scheduled = as.numeric(appointment_day - scheduled_day)
)

# clean up a little bit
data <- filter(data, between(age, 0, 95), hours_since_scheduled >= 0) %>%
  select(-one_of(c("scheduled_day", "appointment_day", "sms_received")))

```

### a. Create train / validation / test sets, cutting the data into 5% - 45% - 50% parts.

First I separated the dataset into a training (5%), validation (45%) and test set (50%).

```{r message=FALSE, warning=FALSE, include=FALSE}

library(h2o)
h2o.init(max_mem_size = "4g")
h2o.no_progress()

my_seed <- 20210318

data <- as.h2o(data)

data_split <- h2o.splitFrame(data, ratios = c(0.05, 0.5), seed = my_seed)

data_train <- data_split[[1]]
data_validation <- data_split[[2]]
data_test <- data_split[[3]]

```

### b. Train a benchmark model of your choice (such as random forest, gbm or glm) and evaluate it on the validation set.

I first trained a linear model as a benchmark model, which performed relatively good results with an AUC of 0.5648 on the training set and a similar AUC of 0.5729 on the validation set.

```{r echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}


y <- "no_show"
X <- setdiff(names(data), y)

# benchmark model

simple_lm <- h2o.glm(
  X, y,
  training_frame = data_train,
  model_id = "logit",
  lambda = 0,
  nfolds = 5,
  seed = my_seed
)
#simple_lm

# AUC (train / cv)
simple_model <- h2o.auc(simple_lm, train = TRUE, xval = TRUE)
# AUC on Validation set
simple_valid <- h2o.auc(h2o.performance(simple_lm, data_validation))

```
 
### c. Build at least 3 models of different families using cross validation, keeping cross validated predictions. You might also try deeplearning.

In the following I train a random forest another linear model and a gradient boosting machine to try to overperform the results of the benchmark model. To achieve a better performance I also tried different tuning parameters, the results (AUCs) on the training dataset ca be found below. As we can see the random forest and glm could not overperform significantly the benchmark model, meanwhile the GBM achived a 0.6 AUC which is well above the benchmark models 0.5729 AUC.

```{r echo=FALSE, warning=FALSE, message=FALSE, cache=TRUE}

# RANDOM FOREST

rf_params <- list(
  ntrees = c(100, 300, 500),    # Number of trees
  mtries = c(2, 4),             # Number of variables
  max_depth = c(5, 10)          # Depth of tree
)

rf_grid <- h2o.grid("randomForest", x = X, y = y,
                    training_frame = data_train,
                    grid_id = "rf",
                    nfolds = 5,
                    seed = my_seed,
                    hyper_params = rf_params,
                    keep_cross_validation_predictions = TRUE
)

# Select the best model based on AUC
rf_model <- h2o.getModel(rf_grid@model_ids[[1]])
# h2o.auc(rf_model)

# GLM

glm_params <- list(alpha = c(0, .25, .5, .75, 1))

# build grid search with previously selected hyperparameters
glm_grid <- h2o.grid("glm", x = X, y = y,
                     grid_id = "glm",
                     training_frame = data_train,
                     lambda_search = TRUE,
                     nfolds = 5,
                     seed = my_seed,
                     hyper_params = glm_params, 
                     keep_cross_validation_predictions = TRUE)

# Select the best model based on AUC
glm_model <- h2o.getModel(glm_grid@model_ids[[1]])
# h2o.auc(glm_model)

# GBM

gbm_params <- list(
  learn_rate = c(0.01, 0.05, 0.1, 0.3),
  ntrees = c(100, 300, 500),
  max_depth = c(2, 5)
)

gbm_grid <- h2o.grid("gbm", x = X, y = y,
                      grid_id = "gbm",
                      training_frame = data_train,
                      nfolds = 5,
                      seed = my_seed,
                      hyper_params = gbm_params, 
                     keep_cross_validation_predictions = TRUE
)

# Select the best model based on AUC
gbm_model <- h2o.getModel(gbm_grid@model_ids[[1]])
# h2o.auc(gbm_model)


# DEEPLEARNING DOES NOT WORK WITH H2O ON MY COMPUTER
# deeplearning_model <- h2o.deeplearning(
#   X, y,
#   training_frame = data_train,
#   seed = my_seed,
#   nfolds = 5,
#   keep_cross_validation_predictions = TRUE
# )


results3c <-
  list("Random_Forest" = h2o.auc(rf_model),
       "GLM" = h2o.auc(glm_model),
       "GBM" = h2o.auc(gbm_model))

results3c <- results3c %>% unlist() %>% as.data.frame() %>% rename("CV AUC" = ".")
#results3c
knitr::kable(results3c,caption="Models AUC on training set")


```
 
### d. Evaluate validation set performance of each model.

Finally I ran every model on the validation set as well, all of them performed well, but the random forest performed better on the validation set compared to its performance on the training set.

```{r echo=FALSE, warning=FALSE, message=FALSE}

rf_valid <- h2o.performance(rf_model, data_validation)
glm_valid <- h2o.performance(glm_model, data_validation)
gbm_valid <- h2o.performance(gbm_model, data_validation)

results3d <-
  list("Random_Forest" = h2o.auc(rf_valid),
       "GLM" = h2o.auc(glm_valid),
       "GBM" = h2o.auc(gbm_valid))

results3d <- results3d %>% unlist() %>% as.data.frame() %>% rename("CV AUC" = ".")
#results3d
knitr::kable(results3d,caption="Models AUC on validation set")

```
 
### e. How large are the correlations of predicted scores of the validation set produced by the base learners?

Let's have a look at the model correlations, we can see below that only the glm and gbm models have some correlations around 80%, but this does not mean that we cannot use them for stacked model.

```{r echo=FALSE, warning=FALSE, message=FALSE}

models <- list(rf_model, glm_model, gbm_model)

h2o.model_correlation_heatmap(models, data_validation)

```
   
### f. Create a stacked ensemble model from the base learners.

Finally I create a stacked ensemble model from the GBM, GLM and RF models. The resulted AUC of the model was 0.6128 which outperformed the other models.

```{r echo=FALSE, warning=FALSE, message=FALSE}
ensemble_model <- h2o.stackedEnsemble(
  X, y,
  training_frame = data_train,
  base_models = models,
  keep_levelone_frame = TRUE
)
ensemble_model

h2o.auc(ensemble_model)
```
 
### g. Evaluate ensembles on validation set. Did it improve prediction?

So the best model became to be the stacked. On the validation it performed slightly worse, but not significantly compared to the others.

```{r echo=FALSE, warning=FALSE, message=FALSE}
results_valid <- map_df(
  c(models, ensemble_model),
  ~{tibble(model = .@model_id, auc = h2o.auc(h2o.performance(., newdata = data_validation)))}
)
knitr::kable(results_valid,caption="Models AUC on validation set")
```
   
### h. Evaluate the best performing model on the test set. How does performance compare to that of the validation set?

The AUC of the was 0.583 which is a slightly worse than the training data AUC.

```{r echo=FALSE, warning=FALSE, message=FALSE}
best_on_test <- map_df(
  c(ensemble_model),
  ~{tibble(model = .@model_id, auc = h2o.auc(h2o.performance(., newdata = data_test)))}
)
```
