---
title: "Data Science 2 - Assignment 2"
author: "Attila Serfozo"
date: '2021.04.07 '
output: html_document
---

# 1. Fashion MNIST

In this project our goal is to build a neural network model able to classify images of fashion items. For model building we will use the keras package

```{r, message=FALSE, warning=FALSE}
path <- "D:/Egyetem/CEU/Winter_Term/Data_Science_2/Assignments/Assignment2"
setwd(path)

# Load packages and data

library(tidyverse)
library(keras)
library(knitr)
my_seed <- 20210406

# Import data
fashion_mnist <- dataset_fashion_mnist()
x_train <- fashion_mnist$train$x
y_train <- fashion_mnist$train$y
x_test <- fashion_mnist$test$x
y_test <- fashion_mnist$test$y
```

## a. Show some example images from the data

Below we can found the first 9 image. We can see that images are in a 28x28 pixel grayscale format resulting 784 pixel variables

```{r, message=FALSE, warning=FALSE, fig.height=10, fig.width=10}
# Save labels into a vector
labels <- c("T-shirt/top",
            "Trouser",
            "Pullover",
            "Dress",
            "Coat",
            "Sandal",
            "Shirt",
            "Sneaker",
            "Bag",
            "Ankle boot")

# Create funtion for getting label names
get_label <- function(num) {
  labels[num+1]
}

# Show some clothes
rotateMatrix <- function(data_row) {
  t(apply(data_row, 2, rev))
}

showClothes <- function(data_row, label,x) {
  num <- label
  image(
    rotateMatrix(data_row),
    col = gray.colors(255), xlab = get_label(num), ylab = ""
  )
}

showFirst9Clothes <- function(data, label) {
  par(mfrow = c(3, 3))
  walk(1:9, ~showClothes(data[.,, ], label[.]))
}

showFirst9Clothes(x_train,y_train)
```

## b. Train a fully connected deep network to predict items

#### Normalize data

The gathered data was in an array fromat, we needed to transform and normalize it.

```{r, message=FALSE, warning=FALSE}
# Normalize the data

x_train <- as.matrix(as.data.frame.array(x_train)) / 255
x_test <- as.matrix(as.data.frame.array(x_test)) / 255

y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
```

In the following I have trained 6 neural network models using 30 epochs with 784 input neurons (each pixel) and 10 output nodes resulting the 10 classification items. I will use the other parameters to experiment with the settings to be able to get a better accuracy. 

#### Deep Network Model 1

For a base model I trained a model with a hidden layer including 128 neurons, relu (Rectified Linear Unit) activation function and 0.3 dropout rate. To receive a validation accuracy to evaluate the model I used the validation_split parameter of the fit function to separate 10% of the data for validation. The resulted accuracy on the validation set is 0.8875. The results and the training history plot can be found below.

```{r, message=FALSE, warning=FALSE}
# Experiment with network architectures and settings (number of hidden layers, number of nodes, activation functions, dropout, etc.)
# Make sure that you use enough epochs so that the validation error starts flattening out 
# provide a plot about the training history (plot(history))

# Model 1

model1 <- keras_model_sequential()
model1 %>%
  layer_dense(units = 128, activation = 'relu', input_shape = c(784)) %>%   # 128 first layer nodes and 784 input features (pixels)
  layer_dropout(rate = 0.3) %>%                                             # 30% of the data gets randomly droped out
  layer_dense(units = 10, activation = 'softmax')                           # 10 nods for the 10 results

compile(
  model1,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# history_1 <- fit(
#   model1, x_train, y_train,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.1
# )
# saveRDS(history_1, paste0(path,"/output/model1.rds"))
history_1 <- readRDS(paste0(path,"/output/model1.rds"))

history_1
plot(history_1)
```

#### Deep Network Model 2

In the second model I tried to increase accuracy by adding more nods in the hidden layer, increasing the number of neurons from 128 to 256. As we can see below it was a good idea, as the accuracy increased to 0.8948.

```{r, message=FALSE, warning=FALSE, cache=TRUE}
# Model 2 increase number of nods

model2 <- keras_model_sequential()
model2 %>%
  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>%  
  layer_dropout(rate = 0.3) %>%   
  layer_dense(units = 10, activation = 'softmax')                           

compile(
  model2,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

history_2 <- fit(
  model2, x_train, y_train,
  epochs = 30, batch_size = 128,
  validation_split = 0.1
)
#saveRDS(history_2, paste0(path,"/output/model2.rds"))
history_2 <- readRDS(paste0(path,"/output/model2.rds"))

history_2
plot(history_2)
```

#### Deep Network Model 3

For my next model I kept the previous model settings and added one more layer with 128 nods. It slightly decreased the validation accuracy to 0.8927, we can notice that the gap between the training and validation accuracy was larger at the previous model so that is maybe overfitting the data more.

```{r, message=FALSE, warning=FALSE}
# Model 3 add another layer

model3 <- keras_model_sequential()
model3 %>%
  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>%   
  layer_dropout(rate = 0.3) %>%     
  layer_dense(units = 128, activation = 'relu', input_shape = c(784)) %>%   
  layer_dropout(rate = 0.3) %>%  
  layer_dense(units = 10, activation = 'softmax')            

compile(
  model3,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# history_3 <- fit(
#   model3, x_train, y_train,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.1
# )
# saveRDS(history_3, paste0(path,"/output/model3.rds"))
history_3 <- readRDS(paste0(path,"/output/model3.rds"))

history_3
plot(history_3)
```

#### Deep Network Model 4

In the fourth model I also tried add one more layer with 128 neurons but this time I added it for the basic model settings. It decreased the accuracy to 0.8887, close the level of the base model.

```{r, message=FALSE, warning=FALSE}
# Model 4 increasing add the second layer with the start parameters 

model4 <- keras_model_sequential()
model4 %>%
  layer_dense(units = 128, activation = 'relu', input_shape = c(784)) %>% 
  layer_dropout(rate = 0.3) %>%            
  layer_dense(units = 128, activation = 'relu', input_shape = c(784)) %>%   
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, activation = 'softmax')          

compile(
  model4,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# history_4 <- fit(
#   model4, x_train, y_train,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.1
# )
# saveRDS(history_4, paste0(path,"/output/model4.rds"))
history_4 <- readRDS(paste0(path,"/output/model4.rds"))

history_4
plot(history_4)
```

#### Deep Network Model 5

In the next model I kept the settings of model 2 as until this time that performed the best on the validation set. But now I lowered the dropout rate to 0.2. Even though it resulted the best accuracy on the training data, the validation set accuracy decreased to 0.891, so maybe this model is overfitting the data. 

```{r, message=FALSE, warning=FALSE}
# Model 5 decreasing the dropout rate

model5 <- keras_model_sequential()
model5 %>%
  layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>%   
  layer_dropout(rate = 0.2) %>%     
  layer_dense(units = 10, activation = 'softmax')            

compile(
  model5,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# history_5 <- fit(
#   model5, x_train, y_train,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.1
# )
# saveRDS(history_5, paste0(path,"/output/model5.rds"))
history_5 <- readRDS(paste0(path,"/output/model5.rds"))

history_5
plot(history_5)
```

#### Deep Network Model 6

In the last network model I still kept the settings of the second model, but I tried changing the relu activation function to softmax. As we can see below it significantly dropped the accuracy on both sets (0.829 on the validation set) so it was not a good idea.

```{r, message=FALSE, warning=FALSE}
# Model 6 changing the activation function

model6 <- keras_model_sequential()
model6 %>%
  layer_dense(units = 256, activation = 'softmax', input_shape = c(784)) %>%   
  layer_dropout(rate = 0.3) %>%     
  layer_dense(units = 10, activation = 'softmax')            

compile(
  model6,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# history_6 <- fit(
#   model6, x_train, y_train,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.1
# )
# saveRDS(history_6, paste0(path,"/output/model6.rds"))
history_6 <- readRDS(paste0(path,"/output/model6.rds"))

history_6
plot(history_6)
```

#### Summary of the results

In the table we can see the models validation accuracy and the best model was the 2nd one with increased number of neurons to 256.

```{r, message=FALSE, warning=FALSE}
# The best model was the 2nd where we increased the number of nods to 256
results_1 <- data.frame(
  model1 = round(history_1$metrics$val_accuracy[30],4),
  model2 = round(history_2$metrics$val_accuracy[30],4),
  model3 = round(history_3$metrics$val_accuracy[30],4),
  model4 = round(history_4$metrics$val_accuracy[30],4),
  model5 = round(history_5$metrics$val_accuracy[30],4),
  model6 = round(history_6$metrics$val_accuracy[30],4)
)
knitr::kable(results_1, caption="Models Validation Accuracy") 
```

## c. Evaluate the model on the test set. How does test error compare to validation error?

Finally, let's have a look at the 2nd model performance on the test set. It performed similarly good as the validation set. The accuracy on the test set is 0.8907 slightly worse than the 0.8948 on the validation set.

```{r, message=FALSE, warning=FALSE}
# test_results <- keras::evaluate(model2, x_test, y_test)
# test_results
```

## d. Try building a convolutional neural network and see if you can improve test set performance

In the following I have trained 5 convolutional neural network models using 30 epochs, all models with a 28,28,1 input shape and a 2D convolutional layer of 3x3 with 32 filters. I will use the other parameters to experiment with the settings to be able to get a better accuracy. 

```{r, message=FALSE, warning=FALSE}
x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))
```

#### CNN Model 1

For a base model I trained a model with a hidden layer including 16 neurons, relu (Rectified Linear Unit) activation function and 0.25 dropout rate. To receive a validation accuracy to evaluate the model I used the validation_split parameter of the fit function to separate 10% of the data for validation. The resulted accuracy on the validation set is 0.9147, which is already much better than the neural net models. The results and the training history plot can be found below. 

```{r, message=FALSE, warning=FALSE}
# CNN Model 1 using the parameters from class

cnn_model_1 <- keras_model_sequential()
cnn_model_1 %>%
  layer_conv_2d(
    filters = 32,               # We do it 32 times 
    kernel_size = c(3, 3),      # It has to take a 3x3 square and evaluate the pixels, do for all possible 3x3
    activation = 'relu',        # 
    input_shape = c(28, 28, 1)  # What kind of input it expects, it is an array/matrix -> this is the image we add
  ) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%    # pooling layer 2x2, it aggregates the 2x2 layer into 1 pixel
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')  # 10 output nodes

compile(
  cnn_model_1,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# cnn_history_1 <- fit(
#   cnn_model_1, x_train, y_train,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.1
# )
# saveRDS(cnn_history_1, paste0(path,"/output/cnn_model1.rds"))
cnn_history_1 <- readRDS(paste0(path,"/output/cnn_model1.rds"))

cnn_history_1
plot(cnn_history_1)
```

#### CNN Model 2

In the second model I changed the nodes size from 16. I tried 32, 64 and 128 number of neurons as well, from which the 32 was the best, which code can be found below. The resulted validation set accuracy increased just a little to 0.917.

```{r, message=FALSE, warning=FALSE, cache=TRUE}
# CNN Model 2 I tried several options for node size (32, 64, 128) and the 32 was the best

cnn_model_2 <- keras_model_sequential()
cnn_model_2 %>%
  layer_conv_2d(
    filters = 32,               
    kernel_size = c(3, 3),      
    activation = 'relu',        
    input_shape = c(28, 28, 1)  
  ) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%    
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

compile(
  cnn_model_2,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

cnn_history_2 <- fit(
  cnn_model_2, x_train, y_train,
  epochs = 30, batch_size = 128,
  validation_split = 0.1
)
# saveRDS(cnn_history_2, paste0(path,"/output/cnn_model2.rds"))
# cnn_history_2 <- readRDS(paste0(path,"/output/cnn_model2.rds"))

cnn_history_2
plot(cnn_history_2)
```

#### CNN Model 3

In the next model I kept the settings of model 2 and tried a lower dropout rate of 0.2. As we can see below it slightly decreased the validation accuracy to 0.9132, but significantly increased the training set accuracy to 0.9504, so there is a chance that it is overfitting the data.

```{r, message=FALSE, warning=FALSE}
# CNN Model 3 try lower dropout rate

cnn_model_3 <- keras_model_sequential()
cnn_model_3 %>%
  layer_conv_2d(
    filters = 32,               
    kernel_size = c(3, 3),      
    activation = 'relu',        
    input_shape = c(28, 28, 1)  
  ) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%    
  layer_dropout(rate = 0.2) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

compile(
  cnn_model_3,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# cnn_history_3 <- fit(
#   cnn_model_3, x_train, y_train,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.1
# )
# saveRDS(cnn_history_3, paste0(path,"/output/cnn_model3.rds"))
cnn_history_3 <- readRDS(paste0(path,"/output/cnn_model3.rds"))

cnn_history_3
plot(cnn_history_3)
```

#### CNN Model 4

In the fourth model I kept the model 2 settings and modified the activation function to softmax from relu. Similarly what we have seen at the neural net models it dropped the accuracy for both sets (0.8948 for validation set).

```{r, message=FALSE, warning=FALSE}
# CNN Model 4 try other activation function

cnn_model_4 <- keras_model_sequential()
cnn_model_4 %>%
  layer_conv_2d(
    filters = 32,               
    kernel_size = c(3, 3),      
    activation = 'relu',        
    input_shape = c(28, 28, 1)  
  ) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%    
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = 'softmax') %>%
  layer_dense(units = 10, activation = 'softmax')

compile(
  cnn_model_4,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# cnn_history_4 <- fit(
#   cnn_model_4, x_train, y_train,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.1
# )
# saveRDS(cnn_history_4, paste0(path,"/output/cnn_model4.rds"))
cnn_history_4 <- readRDS(paste0(path,"/output/cnn_model4.rds"))

cnn_history_4
plot(cnn_history_4)
```

#### CNN Model 5

In the final cnn model I added one more layer with 32 number of nods and 0.25 dropout rate to see whether it can increase further the model accuracy. As we can see the results below it could not beat eather the validation accuracy of model 2, it's accuracy decreased a little to 0.9135.

```{r, message=FALSE, warning=FALSE}
# CNN Model 5 add one more layer

cnn_model_5 <- keras_model_sequential()
cnn_model_5 %>%
  layer_conv_2d(
    filters = 32,               
    kernel_size = c(3, 3),      
    activation = 'relu',        
    input_shape = c(28, 28, 1)  
  ) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%    
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dropout(rate = 0.25) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dense(units = 10, activation = 'softmax')

compile(
  cnn_model_5,
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

# cnn_history_5 <- fit(
#   cnn_model_5, x_train, y_train,
#   epochs = 30, batch_size = 128,
#   validation_split = 0.1
# )
# saveRDS(cnn_history_5, paste0(path,"/output/cnn_model5.rds"))
cnn_history_5 <- readRDS(paste0(path,"/output/cnn_model5.rds"))

cnn_history_5
plot(cnn_history_5)
```

#### Summary of CNN results

In the table we can see the cnn models validation accuracy and the best model was the 2nd one with increased number of neurons to 32, also it became the best model overall taking into account the basic neural network models.

```{r, message=FALSE, warning=FALSE}
# The best model was the 2nd where we increased the number of nods to 32
results_2 <- data.frame(
  cnn_model1 = round(cnn_history_1$metrics$val_accuracy[30],4),
  cnn_model2 = round(cnn_history_2$metrics$val_accuracy[30],4),
  cnn_model3 = round(cnn_history_3$metrics$val_accuracy[30],4),
  cnn_model4 = round(cnn_history_4$metrics$val_accuracy[30],4),
  cnn_model5 = round(cnn_history_5$metrics$val_accuracy[30],4)
)
knitr::kable(results_2, caption="CNN Models Validation Accuracy") 
```

Finally, econd convolutional neural network model performed similarly good on the test set with a loss of 0.2886 and an accuracy of 0.9148. The test set error decreased slightly compared to the validation set's error of 0.917.

```{r, message=FALSE, warning=FALSE}
# test_results_cnn <- keras::evaluate(cnn_model_2, x_test, y_test)
# test_results_cnn
```